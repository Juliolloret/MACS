"""Fake LLM implementation used solely for testing."""

import json
from typing import List, Optional
from cache import Cache


class FakeEmbeddings:
    """Fake embeddings client for testing."""

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Return a list of fake embeddings."""
        return [[0.1, 0.2, 0.3] for _ in texts]


class FakeLLM:
    """Minimal in-memory LLM used for testing."""

    def __init__(self, app_config=None, response_map=None, cache: Optional[Cache] = None):
        """Create a fake client with optional canned responses."""
        self.app_config = app_config
        self.client = self
        self.response_map = response_map or {}
        self._embedding_client = FakeEmbeddings()
        self._cache = cache or Cache()
        self.last_token_usage = 0
        self.total_tokens_used = 0
        self.last_prompt: Optional[str] = None
        self.last_system_message: Optional[str] = None
        self.last_completion_extra = None

    def get_response(
        self,
        system_message,
        user_message,
        model,
        temperature,
    ):  # pylint: disable=unused-argument
        """Return a deterministic response for ``user_message``."""
        if user_message in self.response_map:
            return self.response_map[user_message]
        if "generate hypotheses" in user_message.lower() or "propose" in user_message.lower():
            return json.dumps({
                "key_opportunities": "This is a fake opportunity generated for testing by FakeLLM.",
                "hypotheses": [
                    {
                        "hypothesis": "This is a fake test hypothesis generated by FakeLLM.",
                        "justification": "This hypothesis is designed for testing the application.",
                    }
                ]
            })
        return "[FAKE] ok"

    def complete(self, system, prompt, model, temperature=0.1, extra=None):
        """Simulate a completion call with simple caching."""
        self.last_system_message = system
        self.last_prompt = prompt
        self.last_completion_extra = extra
        key = self._cache.make_key(model, system, prompt, temperature, extra)
        cached = self._cache.get(key)
        if cached is not None:
            self.last_token_usage = 0
            return cached
        result = self.get_response(system, prompt, model, temperature)
        self._cache.set(key, result)
        self.last_token_usage = len(prompt.split())
        self.total_tokens_used += self.last_token_usage
        return result

    def get_embeddings_client(self):
        """Return a dummy embeddings client."""
        return self._embedding_client

    def get_last_token_usage(self):
        """Return token count from the most recent completion."""
        return self.last_token_usage

    def get_total_tokens_used(self):
        """Return cumulative token usage for this client."""
        return self.total_tokens_used

    def close(self) -> None:
        """No-op close method for API compatibility."""
        return None
