from typing import Optional, Dict
from llm import LLMClient

from typing import Any

class FakeLLM(LLMClient):
    def __init__(self, app_config: Dict[str, Any], scripted_outputs: Optional[Dict[str, str]] = None, api_key: Optional[str] = None, timeout: int = 120):
        self.scripted = scripted_outputs or {}
        self.last_prompt = None
        self.app_config = app_config
    def complete(self, *, system: str, prompt:str,
                 model: Optional[str] = None,
                 temperature: Optional[float] = None,
                 extra: Optional[Dict] = None) -> str:
        self.last_prompt = prompt

        # Check if the system prompt is asking for a JSON output for hypotheses
        if "**Output Format STRICTLY REQUIRED:**" in system and "hypotheses" in system:
            return """
{
  "key_opportunities": "This is a fake opportunity generated for testing by FakeLLM.",
  "hypotheses": [
    {
      "hypothesis": "This is a fake test hypothesis generated by FakeLLM.",
      "justification": "This hypothesis is designed for testing the application workflow and ensuring the JSON parsing and subsequent steps function correctly."
    }
  ]
}
"""
        # Default fake response
        return self.scripted.get(prompt, "[FAKE] ok")
